{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "class reader(object):\n",
    "  def __init__(self, data_file = \"./data/wdbc.data\"):\n",
    "    self.value = []\n",
    "    with open(data_file, \"rb\") as f:\n",
    "      csv_reader = csv.reader(f, delimiter=\",\")\n",
    "      for i, row in enumerate(csv_reader):\n",
    "        self.value.append(row)\n",
    "    # print self.value[0]\n",
    "    # print self.value[0][0]\n",
    "    # print self.value[0][1]\n",
    "\n",
    "    self.raw_to_vector(self.value)\n",
    "    self.split(num_validation_examples=101)\n",
    "    \n",
    "#     print self.id.shape\n",
    "#     print self.x.shape\n",
    "#     print self.y.shape\n",
    "    \n",
    "    self.num_examples = len(self.x_train)\n",
    "    self.start_index = 0\n",
    "    self.shuffle_indices = range(self.num_examples)\n",
    "\n",
    "    self.num_examples_val = len(self.x_val) # = 114 (validation_data)\n",
    "    self.start_index_val = 0\n",
    "    self.shuffle_indices_val = range(self.num_examples_val)\n",
    "    self.num_examples_val = 101\n",
    "\n",
    "  def raw_to_vector(self, value):\n",
    "    self.id = []\n",
    "    self.x = []\n",
    "    self.y = []\n",
    "\n",
    "    for row in self.value:\n",
    "        x = np.zeros(30)\n",
    "        for i in range(30):\n",
    "            x[i] = float(row[i+2])\n",
    "        if row[1] == \"B\":\n",
    "            y = 0\n",
    "        else:\n",
    "            y = 1\n",
    "        self.x.append(x)\n",
    "        self.y.append(y)\n",
    "        id = int(row[0])\n",
    "        self.id.append(id)\n",
    "        \n",
    "    self.x, self.y, self.id = np.array(self.x), np.array(self.y), np.array(self.id)\n",
    "\n",
    "  def split(self, num_validation_examples):\n",
    "    self.x_train = self.x[ num_validation_examples: ]\n",
    "    self.x_val = self.x[ : num_validation_examples ]\n",
    "\n",
    "    self.y_train = self.y[ num_validation_examples: ]\n",
    "    self.y_val = self.y[ : num_validation_examples ]\n",
    "\n",
    "    self.id_train = self.id[ num_validation_examples: ]\n",
    "    self.id_val = self.id[ :num_validation_examples ]\n",
    "\n",
    "  def next_batch(self, batch_size, split=\"train\"):\n",
    "\n",
    "    if split == \"train\":\n",
    "      if self.start_index == 0:\n",
    "        np.random.shuffle(self.shuffle_indices) # shuffle indices\n",
    "\n",
    "      end_index = min([self.num_examples, self.start_index + batch_size])\n",
    "      batch_indices = [ self.shuffle_indices[idx] for idx in range(self.start_index, end_index) ]\n",
    "\n",
    "      batch_x = self.x_train[ batch_indices ]\n",
    "      batch_y = self.y_train[ batch_indices ]\n",
    "      batch_id = self.id_train[ batch_indices ] \n",
    "\n",
    "      if end_index == self.num_examples:\n",
    "        self.start_index = 0\n",
    "      else: self.start_index = end_index\n",
    "\n",
    "      return batch_x, batch_y\n",
    "\n",
    "    elif split == \"val\":\n",
    "      if self.start_index_val == 0:\n",
    "        np.random.shuffle(self.shuffle_indices_val) # shuffle indices\n",
    "\n",
    "      end_index = min([self.num_examples_val, self.start_index_val + batch_size])\n",
    "      batch_indices = [ self.shuffle_indices_val[idx] for idx in range(self.start_index_val, end_index) ]\n",
    "\n",
    "      batch_x = self.x_val[ batch_indices ]\n",
    "      batch_y = self.y_val[ batch_indices ]\n",
    "      batch_id = self.id_val[ batch_indices ] \n",
    "\n",
    "      if end_index == self.num_examples_val:\n",
    "        self.start_index_val = 0\n",
    "      else: self.start_index_val = end_index\n",
    "\n",
    "      return batch_x, batch_y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.1\n",
    "num_steps = 10000\n",
    "batch_size = 117\n",
    "display_step = 100\n",
    "\n",
    "data1 = reader()\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of neurons\n",
    "n_hidden_2 = 256 # 2nd layer number of neurons\n",
    "num_input = 30 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 2 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "def neural_net(x):\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct model\n",
    "logits = neural_net(X)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 4426625.0000, Training Accuracy= 0.376\n",
      "Step 100, Minibatch Loss= 1068.7726, Training Accuracy= 0.726\n",
      "Step 200, Minibatch Loss= 589.0905, Training Accuracy= 0.538\n",
      "Step 300, Minibatch Loss= 231.3302, Training Accuracy= 0.573\n",
      "Step 400, Minibatch Loss= 477.1205, Training Accuracy= 0.385\n",
      "Step 500, Minibatch Loss= 151.7283, Training Accuracy= 0.573\n",
      "Step 600, Minibatch Loss= 154.0618, Training Accuracy= 0.650\n",
      "Step 700, Minibatch Loss= 262.0178, Training Accuracy= 0.615\n",
      "Step 800, Minibatch Loss= 54.2917, Training Accuracy= 0.684\n",
      "Step 900, Minibatch Loss= 41.1701, Training Accuracy= 0.538\n",
      "Step 1000, Minibatch Loss= 23.9615, Training Accuracy= 0.658\n",
      "Step 1100, Minibatch Loss= 34.3410, Training Accuracy= 0.607\n",
      "Step 1200, Minibatch Loss= 55.8856, Training Accuracy= 0.444\n",
      "Step 1300, Minibatch Loss= 30.0417, Training Accuracy= 0.641\n",
      "Step 1400, Minibatch Loss= 52.3907, Training Accuracy= 0.479\n",
      "Step 1500, Minibatch Loss= 176205.8125, Training Accuracy= 0.325\n",
      "Step 1600, Minibatch Loss= 11667.3740, Training Accuracy= 0.684\n",
      "Step 1700, Minibatch Loss= 3276.7356, Training Accuracy= 0.564\n",
      "Step 1800, Minibatch Loss= 829.0258, Training Accuracy= 0.607\n",
      "Step 1900, Minibatch Loss= 837.3900, Training Accuracy= 0.564\n",
      "Step 2000, Minibatch Loss= 1256.5692, Training Accuracy= 0.504\n",
      "Step 2100, Minibatch Loss= 598.0271, Training Accuracy= 0.718\n",
      "Step 2200, Minibatch Loss= 487.9369, Training Accuracy= 0.632\n",
      "Step 2300, Minibatch Loss= 270.9271, Training Accuracy= 0.547\n",
      "Step 2400, Minibatch Loss= 353.3996, Training Accuracy= 0.607\n",
      "Step 2500, Minibatch Loss= 626.5740, Training Accuracy= 0.564\n",
      "Step 2600, Minibatch Loss= 336.0253, Training Accuracy= 0.564\n",
      "Step 2700, Minibatch Loss= 70.2437, Training Accuracy= 0.735\n",
      "Step 2800, Minibatch Loss= 62.4403, Training Accuracy= 0.462\n",
      "Step 2900, Minibatch Loss= 62.3353, Training Accuracy= 0.607\n",
      "Step 3000, Minibatch Loss= 49.1517, Training Accuracy= 0.530\n",
      "Step 3100, Minibatch Loss= 35.1062, Training Accuracy= 0.556\n",
      "Step 3200, Minibatch Loss= 39.3696, Training Accuracy= 0.607\n",
      "Step 3300, Minibatch Loss= 43.9762, Training Accuracy= 0.573\n",
      "Step 3400, Minibatch Loss= 24.6499, Training Accuracy= 0.513\n",
      "Step 3500, Minibatch Loss= 33.5226, Training Accuracy= 0.598\n",
      "Step 3600, Minibatch Loss= 22.4939, Training Accuracy= 0.641\n",
      "Step 3700, Minibatch Loss= 27.7548, Training Accuracy= 0.530\n",
      "Step 3800, Minibatch Loss= 10.6345, Training Accuracy= 0.658\n",
      "Step 3900, Minibatch Loss= 8.5984, Training Accuracy= 0.632\n",
      "Step 4000, Minibatch Loss= 11.6952, Training Accuracy= 0.641\n",
      "Step 4100, Minibatch Loss= 11.6482, Training Accuracy= 0.462\n",
      "Step 4200, Minibatch Loss= 7.6826, Training Accuracy= 0.556\n",
      "Step 4300, Minibatch Loss= 7.0960, Training Accuracy= 0.462\n",
      "Step 4400, Minibatch Loss= 5.1486, Training Accuracy= 0.521\n",
      "Step 4500, Minibatch Loss= 231.2202, Training Accuracy= 0.325\n",
      "Step 4600, Minibatch Loss= 1771.5450, Training Accuracy= 0.701\n",
      "Step 4700, Minibatch Loss= 53357.6406, Training Accuracy= 0.274\n",
      "Step 4800, Minibatch Loss= 19.5735, Training Accuracy= 0.641\n",
      "Step 4900, Minibatch Loss= 8.2149, Training Accuracy= 0.496\n",
      "Step 5000, Minibatch Loss= 12.8561, Training Accuracy= 0.556\n",
      "Step 5100, Minibatch Loss= 6.9537, Training Accuracy= 0.564\n",
      "Step 5200, Minibatch Loss= 4.8509, Training Accuracy= 0.573\n",
      "Step 5300, Minibatch Loss= 3.0985, Training Accuracy= 0.530\n",
      "Step 5400, Minibatch Loss= 2.1723, Training Accuracy= 0.641\n",
      "Step 5500, Minibatch Loss= 2.9024, Training Accuracy= 0.598\n",
      "Step 5600, Minibatch Loss= 4.3446, Training Accuracy= 0.436\n",
      "Step 5700, Minibatch Loss= 4.6453, Training Accuracy= 0.615\n",
      "Step 5800, Minibatch Loss= 1.4641, Training Accuracy= 0.564\n",
      "Step 5900, Minibatch Loss= 2.8356, Training Accuracy= 0.462\n",
      "Step 6000, Minibatch Loss= 2.3344, Training Accuracy= 0.538\n",
      "Step 6100, Minibatch Loss= 1.3156, Training Accuracy= 0.650\n",
      "Step 6200, Minibatch Loss= 2.2025, Training Accuracy= 0.513\n",
      "Step 6300, Minibatch Loss= 2.3604, Training Accuracy= 0.538\n",
      "Step 6400, Minibatch Loss= 0.8562, Training Accuracy= 0.624\n",
      "Step 6500, Minibatch Loss= 2.9326, Training Accuracy= 0.650\n",
      "Step 6600, Minibatch Loss= 366.2115, Training Accuracy= 0.325\n",
      "Step 6700, Minibatch Loss= 39.3311, Training Accuracy= 0.632\n",
      "Step 6800, Minibatch Loss= 26.4796, Training Accuracy= 0.265\n",
      "Step 6900, Minibatch Loss= 76.1717, Training Accuracy= 0.308\n",
      "Step 7000, Minibatch Loss= 988.3081, Training Accuracy= 0.325\n",
      "Step 7100, Minibatch Loss= 252.4881, Training Accuracy= 0.325\n",
      "Step 7200, Minibatch Loss= 2838.9749, Training Accuracy= 0.333\n",
      "Step 7300, Minibatch Loss= 27454.3828, Training Accuracy= 0.641\n",
      "Step 7400, Minibatch Loss= 305.1658, Training Accuracy= 0.692\n",
      "Step 7500, Minibatch Loss= 715.6866, Training Accuracy= 0.504\n",
      "Step 7600, Minibatch Loss= 347.1087, Training Accuracy= 0.487\n",
      "Step 7700, Minibatch Loss= 301.0347, Training Accuracy= 0.684\n",
      "Step 7800, Minibatch Loss= 179.5816, Training Accuracy= 0.556\n",
      "Step 7900, Minibatch Loss= 189.4115, Training Accuracy= 0.573\n",
      "Step 8000, Minibatch Loss= 379.6708, Training Accuracy= 0.504\n",
      "Step 8100, Minibatch Loss= 203.7071, Training Accuracy= 0.521\n",
      "Step 8200, Minibatch Loss= 163.7512, Training Accuracy= 0.538\n",
      "Step 8300, Minibatch Loss= 96.4318, Training Accuracy= 0.615\n",
      "Step 8400, Minibatch Loss= 220.6280, Training Accuracy= 0.513\n",
      "Step 8500, Minibatch Loss= 55.5012, Training Accuracy= 0.709\n",
      "Step 8600, Minibatch Loss= 34.7254, Training Accuracy= 0.735\n",
      "Step 8700, Minibatch Loss= 25.4960, Training Accuracy= 0.573\n",
      "Step 8800, Minibatch Loss= 62.7895, Training Accuracy= 0.556\n",
      "Step 8900, Minibatch Loss= 26.7634, Training Accuracy= 0.521\n",
      "Step 9000, Minibatch Loss= 39.1927, Training Accuracy= 0.513\n",
      "Step 9100, Minibatch Loss= 8.5815, Training Accuracy= 0.590\n",
      "Step 9200, Minibatch Loss= 8.4600, Training Accuracy= 0.513\n",
      "Step 9300, Minibatch Loss= 18.0439, Training Accuracy= 0.547\n",
      "Step 9400, Minibatch Loss= 11.5042, Training Accuracy= 0.615\n",
      "Step 9500, Minibatch Loss= 13.1765, Training Accuracy= 0.564\n",
      "Step 9600, Minibatch Loss= 13.2078, Training Accuracy= 0.641\n",
      "Step 9700, Minibatch Loss= 12.6815, Training Accuracy= 0.641\n",
      "Step 9800, Minibatch Loss= 9.1653, Training Accuracy= 0.521\n",
      "Step 9900, Minibatch Loss= 5.1264, Training Accuracy= 0.487\n",
      "Step 10000, Minibatch Loss= 4.1387, Training Accuracy= 0.667\n",
      "Optimization Finished!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (10000, 784) for Tensor u'Placeholder_8:0', which has shape '(?, 30)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-4757757b4f0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Calculate accuracy for MNIST test images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     print(\"Testing Accuracy:\",         sess.run(accuracy, feed_dict={X: mnist.test.images,\n\u001b[0;32m---> 22\u001b[0;31m                                       Y: mnist.test.labels}))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    973\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m    976\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (10000, 784) for Tensor u'Placeholder_8:0', which has shape '(?, 30)'"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, num_steps+1):\n",
    "        batch_x = data1.next_batch(batch_size, split=\"train\")[0]\n",
    "        batch_y = OneHotEncoder(sparse=False).fit_transform(data1.next_batch(batch_size, split=\"train\")[1].reshape(len(a), 1))\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for MNIST test images\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: mnist.test.images,\n",
    "                                      Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = data1.next_batch(batch_size, split=\"train\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "integer_encoded1 = a.reshape(len(a), 1)\n",
    "onehot_encoded1 = OneHotEncoder(sparse=False).fit_transform(integer_encoded1)\n",
    "print (onehot_encoded1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cold' 'cold' 'warm' 'cold' 'hot' 'hot' 'warm' 'cold' 'warm' 'hot']\n",
      "[0 0 2 0 1 1 2 0 2 1]\n",
      "[[ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define example\n",
    "data = ['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', 'warm', 'hot']\n",
    "values = array(data)\n",
    "print(values)\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "print(integer_encoded)\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [0],\n",
       "       [2],\n",
       "       [1]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "integer_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 2)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OneHotEncoder(sparse=False).fit_transform(data1.next_batch(batch_size, split=\"train\")[1].reshape(len(a), 1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.next_batch(batch_size, split=\"train\")[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
