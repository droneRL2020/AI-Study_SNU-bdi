{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "class test_reader(object):\n",
    "  def __init__(self, data_file=\"./data/test.csv\"):\n",
    "    self.value = []\n",
    "    with open(data_file, \"rb\") as f:\n",
    "      csv_reader = csv.reader(f, delimiter=\",\")\n",
    "      \n",
    "      for i, row in enumerate(csv_reader):\n",
    "        if i == 0:\n",
    "          self.attribute = row\n",
    "          continue\n",
    "\n",
    "        self.value.append(row)\n",
    "    \n",
    "    self.raw_to_vector(self.value)\n",
    "   \n",
    "  def raw_to_vector(self, value):\n",
    "    self.x = []\n",
    "    self.id = []\n",
    "    for i, row in enumerate(self.value):\n",
    "      x = np.zeros(6)\n",
    "\n",
    "      x[0] = float(row[2-1]) # Pclass\n",
    "      x[1] = 0 if row[4-1] == \"male\" else 1 # Sex\n",
    "      try: x[2] = float(row[5-1]) # Age\n",
    "      except: x[2] = 20.0\n",
    "      x[3] = float(row[6-1]) # SibSp\n",
    "      x[4] = float(row[7-1]) # Parch\n",
    "      try: x[5] = float(row[9-1]) # Fare\n",
    "      except: x[5] = 0.0 # If not fare exists, initialize it to zero\n",
    "\n",
    "      id = int(row[0]) # Passenger ID\n",
    "\n",
    "      self.x.append(x)\n",
    "      self.id.append(id)\n",
    "\n",
    "    self.x, self.id = np.array(self.x), np.array(self.id)\n",
    "\n",
    "  def full_batch(self):\n",
    "    return self.x, self.id\n",
    "\n",
    "\n",
    "class reader(object):\n",
    "  def __init__(self, data_file = \"./data/train.csv\"):\n",
    "    self.value = []\n",
    "    with open(data_file, \"rb\") as f:\n",
    "      csv_reader = csv.reader(f, delimiter=\",\")\n",
    "      \n",
    "      for i, row in enumerate(csv_reader):\n",
    "        if i == 0:\n",
    "          self.attribute = row\n",
    "          continue\n",
    "\n",
    "        self.value.append(row)\n",
    "    \n",
    "    self.raw_to_vector(self.value)\n",
    "    self.split(num_validation_examples=100)\n",
    "\n",
    "    # self.x_train.shape = (791, 6)\n",
    "    # self.y_train.shape = (791)\n",
    "    # self.id_train.shape = (791)\n",
    "    \n",
    "    # self.x_val.shape = (100, 6)\n",
    "    # self.y_val.shape = (100)\n",
    "    # self.id_val.shape = (100)\n",
    "\n",
    "    self.num_examples = len(self.x_train) # = 791 (train_data)\n",
    "    self.start_index = 0\n",
    "    self.shuffle_indices = range(self.num_examples)\n",
    "\n",
    "    self.num_examples_val = len(self.x_val) # = 100 (validation_data)\n",
    "    self.start_index_val = 0\n",
    "    self.shuffle_indices_val = range(self.num_examples_val)\n",
    "\n",
    "  def raw_to_vector(self, value):\n",
    "    self.x = []\n",
    "    self.y = []\n",
    "    self.id = []\n",
    "    for i, row in enumerate(self.value):\n",
    "      x = np.zeros(6)\n",
    "\n",
    "      x[0] = float(row[2]) # Pclass\n",
    "      x[1] = 0 if row[4] == \"male\" else 1 # Sex\n",
    "      try: x[2] = float(row[5]) # Age\n",
    "      except: x[2] = 20.0\n",
    "      x[3] = float(row[6]) # SibSp\n",
    "      x[4] = float(row[7]) # Parch\n",
    "      x[5] = float(row[9]) # Fare\n",
    "\n",
    "      y = int(row[1]) # Survived = (0, 1)\n",
    "      id = int(row[0]) # Passenger ID\n",
    "\n",
    "#       if i < 10:\n",
    "#         print \"x: \", x, \"/shape: \", np.array(x).shape\n",
    "#         print \"y: \", y, \"/shape: \", np.array(y).shape\n",
    "#         print \"id: \", id, \"/shape: \", np.array(id).shape\n",
    "\n",
    "      self.x.append(x)\n",
    "      self.y.append(y)\n",
    "      self.id.append(id)\n",
    "\n",
    "    self.x, self.y, self.id = np.array(self.x), np.array(self.y), np.array(self.id)\n",
    "\n",
    "  def split(self, num_validation_examples):\n",
    "    self.x_train = self.x[ num_validation_examples: ]\n",
    "    self.x_val = self.x[ : num_validation_examples ]\n",
    "\n",
    "    self.y_train = self.y[ num_validation_examples: ]\n",
    "    self.y_val = self.y[ : num_validation_examples ]\n",
    "\n",
    "    self.id_train = self.id[ num_validation_examples: ]\n",
    "    self.id_val = self.id[ :num_validation_examples ]\n",
    "\n",
    "  def next_batch(self, batch_size, split=\"train\"):\n",
    "\n",
    "    if split == \"train\":\n",
    "      if self.start_index == 0:\n",
    "        np.random.shuffle(self.shuffle_indices) # shuffle indices\n",
    "\n",
    "      end_index = min([self.num_examples, self.start_index + batch_size])\n",
    "      batch_indices = [ self.shuffle_indices[idx] for idx in range(self.start_index, end_index) ]\n",
    "\n",
    "      batch_x = self.x_train[ batch_indices ]\n",
    "      batch_y = self.y_train[ batch_indices ]\n",
    "      batch_id = self.id_train[ batch_indices ] \n",
    "\n",
    "      if end_index == self.num_examples:\n",
    "        self.start_index = 0\n",
    "      else: self.start_index = end_index\n",
    "\n",
    "      return batch_x, batch_y, batch_id \n",
    "\n",
    "    elif split == \"val\":\n",
    "      if self.start_index_val == 0:\n",
    "        np.random.shuffle(self.shuffle_indices_val) # shuffle indices\n",
    "\n",
    "      end_index = min([self.num_examples_val, self.start_index_val + batch_size])\n",
    "      batch_indices = [ self.shuffle_indices_val[idx] for idx in range(self.start_index_val, end_index) ]\n",
    "\n",
    "      batch_x = self.x_val[ batch_indices ]\n",
    "      batch_y = self.y_val[ batch_indices ]\n",
    "      batch_id = self.id_val[ batch_indices ] \n",
    "\n",
    "      if end_index == self.num_examples_val:\n",
    "        self.start_index_val = 0\n",
    "      else: self.start_index_val = end_index\n",
    "\n",
    "      return batch_x, batch_y, batch_id  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[   2.    ,    0.    ,   18.    ,    0.    ,    0.    ,   11.5   ],\n",
       "        [   2.    ,    1.    ,   57.    ,    0.    ,    0.    ,   10.5   ],\n",
       "        [   3.    ,    0.    ,    6.    ,    0.    ,    1.    ,   12.475 ],\n",
       "        [   3.    ,    0.    ,   24.    ,    0.    ,    0.    ,    7.05  ],\n",
       "        [   3.    ,    0.    ,   20.    ,    8.    ,    2.    ,   69.55  ],\n",
       "        [   2.    ,    1.    ,   50.    ,    0.    ,    0.    ,   10.5   ],\n",
       "        [   1.    ,    0.    ,   34.    ,    0.    ,    0.    ,   26.55  ],\n",
       "        [   3.    ,    0.    ,   34.    ,    0.    ,    0.    ,    8.05  ],\n",
       "        [   1.    ,    0.    ,   19.    ,    1.    ,    0.    ,   53.1   ],\n",
       "        [   1.    ,    1.    ,   35.    ,    1.    ,    0.    ,   90.    ],\n",
       "        [   1.    ,    1.    ,   44.    ,    0.    ,    0.    ,   27.7208],\n",
       "        [   2.    ,    1.    ,   20.    ,    0.    ,    0.    ,   12.35  ],\n",
       "        [   2.    ,    0.    ,   20.    ,    0.    ,    0.    ,    0.    ],\n",
       "        [   2.    ,    0.    ,   26.    ,    0.    ,    0.    ,   10.5   ],\n",
       "        [   1.    ,    0.    ,   48.    ,    1.    ,    0.    ,   76.7292],\n",
       "        [   1.    ,    1.    ,   19.    ,    0.    ,    2.    ,   26.2833],\n",
       "        [   3.    ,    0.    ,   20.    ,    0.    ,    0.    ,    9.5   ],\n",
       "        [   2.    ,    0.    ,   25.    ,    1.    ,    2.    ,   41.5792],\n",
       "        [   1.    ,    0.    ,   29.    ,    1.    ,    0.    ,   66.6   ],\n",
       "        [   1.    ,    0.    ,   18.    ,    1.    ,    0.    ,  108.9   ],\n",
       "        [   2.    ,    1.    ,   24.    ,    2.    ,    3.    ,   18.75  ],\n",
       "        [   3.    ,    0.    ,   24.    ,    0.    ,    0.    ,    7.4958],\n",
       "        [   3.    ,    0.    ,   20.    ,    0.    ,    0.    ,    7.3125],\n",
       "        [   1.    ,    0.    ,   31.    ,    0.    ,    0.    ,   50.4958],\n",
       "        [   2.    ,    1.    ,   44.    ,    1.    ,    0.    ,   26.    ],\n",
       "        [   3.    ,    0.    ,   20.    ,    0.    ,    0.    ,    7.775 ],\n",
       "        [   2.    ,    1.    ,   28.    ,    1.    ,    0.    ,   26.    ],\n",
       "        [   2.    ,    0.    ,   34.    ,    1.    ,    0.    ,   21.    ],\n",
       "        [   3.    ,    0.    ,   24.    ,    0.    ,    0.    ,    7.7958],\n",
       "        [   2.    ,    0.    ,   16.    ,    0.    ,    0.    ,   10.5   ],\n",
       "        [   1.    ,    0.    ,   42.    ,    1.    ,    0.    ,   52.5542],\n",
       "        [   1.    ,    0.    ,   46.    ,    0.    ,    0.    ,   79.2   ]]),\n",
       " array([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 1, 0]),\n",
       " array([758, 773, 752, 211, 202, 527, 448, 462, 749, 487, 195, 304, 482,\n",
       "        620, 646, 137, 869, 686, 337, 506, 438, 515, 155, 868, 855, 668,\n",
       "        427, 477, 500, 842, 622, 790]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = reader()\n",
    "a.next_batch(32, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class dnn(object):\n",
    "  def __init__(self):\n",
    "    # Define model's input as tf.placeholder\n",
    "    self.x = tf.placeholder(dtype=tf.float32, shape=[None, 6])\n",
    "    self.y = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "\n",
    "    \n",
    "    # Define parameters for 3-layers dnn model\n",
    "    self.w_1, self.b_1 = tf.get_variable(name=\"w_1\", shape=[6, 50]), tf.get_variable(name=\"b_1\", shape=[50])\n",
    "    self.w_2, self.b_2 = tf.get_variable(name=\"w_2\", shape=[50, 50]), tf.get_variable(name=\"b_2\", shape=[50])\n",
    "    self.w_3, self.b_3 = tf.get_variable(name=\"w_3\", shape=[50, 2]), tf.get_variable(name=\"b_3\", shape=[2])\n",
    "\n",
    "\n",
    "    # build graph for forward & backward propagation\n",
    "    # Also, build_graph() returns scalar and histogram\n",
    "    self.scalar_summaries, self.histogram_summaries = self.build_graph()\n",
    "\n",
    "  def build_graph(self):\n",
    "\n",
    "    # h1.shape = (batch, 50)\n",
    "    h1 = tf.matmul(self.x, self.w_1) + self.b_1 \n",
    "    h1 = tf.nn.relu(h1)\n",
    "\n",
    "    # h2.shape = (batch, 50)\n",
    "    h2 = tf.matmul(h1, self.w_2) + self.b_2\n",
    "    h2 = tf.nn.relu(h2)\n",
    "\n",
    "    # h3.shape = (batch, 2)\n",
    "    h3 = tf.matmul(h2, self.w_3) + self.b_3\n",
    "    h3 = tf.nn.softmax(h3, dim=-1)\n",
    "\n",
    "    prediction = h3\n",
    "\n",
    "    y_onehot = tf.one_hot(\n",
    "      indices=self.y,\n",
    "      depth=2,\n",
    "      on_value=1.0,\n",
    "      off_value=0.0,\n",
    "      )\n",
    "\n",
    "    # We use L2 loss as cost function and average the batch's loss\n",
    "    self.loss = tf.reduce_mean((y_onehot - prediction) * (y_onehot - prediction))\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    \n",
    "    self.train_op = optimizer.minimize(self.loss)\n",
    "\n",
    "    pred_index = tf.cast(tf.argmax(prediction, dimension=1), tf.int32)\n",
    "    correct_prediction = tf.equal(pred_index, self.y)\n",
    "    self.acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "    # Save tensors that would be shown on TensorBoard as list \n",
    "    scalar_summaries = [(self.loss, \"loss\"), (self.acc, \"accuracy\")]\n",
    "    histogram_summaries = [(h1, \"h1\"), (h2, \"h2\"), (h3, \"h3\"), (self.w_1, \"w_1\"), (self.b_1, \"b_1\"), (self.w_2, \"w_2\"), (self.b_2, \"b_2\"), (self.w_3, \"w_3\"), (self.b_3, \"b_3\")]\n",
    "\n",
    "    return scalar_summaries, histogram_summaries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def summary_register(scalar_summaries, histogram_summaries):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    scalar_summaries: List of tensors that would be shown to tensorboard as scalar graph\n",
    "    histogram_summaries: List of tensors that would be shown to tensorboard as histogram    graph\n",
    "\n",
    "  Returns:\n",
    "    It only register summary information on TensorFlow Graph, returns nothing.\n",
    "  \"\"\"\n",
    "  for scalar_tensor, name in scalar_summaries:\n",
    "    tf.summary.scalar(name, scalar_tensor)\n",
    "\n",
    "  for histogram_tensor, name in histogram_summaries:\n",
    "    tf.summary.histogram(name, histogram_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| steps 0000000 | loss: 0.260\n",
      "| steps 0000000 | Validation Accuracy: 0.500\n",
      "| steps 0000100 | loss: 0.249\n",
      "| steps 0000100 | Validation Accuracy: 0.530\n",
      "| steps 0000200 | loss: 0.212\n",
      "| steps 0000200 | Validation Accuracy: 0.530\n",
      "| steps 0000300 | loss: 0.136\n",
      "| steps 0000300 | Validation Accuracy: 0.500\n",
      "| steps 0000400 | loss: 0.135\n",
      "| steps 0000400 | Validation Accuracy: 0.530\n",
      "| steps 0000500 | loss: 0.216\n",
      "| steps 0000500 | Validation Accuracy: 0.560\n",
      "| steps 0000600 | loss: 0.118\n",
      "| steps 0000600 | Validation Accuracy: 0.550\n",
      "| steps 0000700 | loss: 0.228\n",
      "| steps 0000700 | Validation Accuracy: 0.530\n",
      "| steps 0000800 | loss: 0.195\n",
      "| steps 0000800 | Validation Accuracy: 0.550\n",
      "| steps 0000900 | loss: 0.266\n",
      "| steps 0000900 | Validation Accuracy: 0.580\n",
      "| steps 0001000 | loss: 0.232\n",
      "| steps 0001000 | Validation Accuracy: 0.620\n",
      "| steps 0001100 | loss: 0.153\n",
      "| steps 0001100 | Validation Accuracy: 0.550\n",
      "| steps 0001200 | loss: 0.216\n",
      "| steps 0001200 | Validation Accuracy: 0.620\n",
      "| steps 0001300 | loss: 0.072\n",
      "| steps 0001300 | Validation Accuracy: 0.580\n",
      "| steps 0001400 | loss: 0.179\n",
      "| steps 0001400 | Validation Accuracy: 0.640\n",
      "| steps 0001500 | loss: 0.207\n",
      "| steps 0001500 | Validation Accuracy: 0.590\n",
      "| steps 0001600 | loss: 0.257\n",
      "| steps 0001600 | Validation Accuracy: 0.660\n",
      "| steps 0001700 | loss: 0.200\n",
      "| steps 0001700 | Validation Accuracy: 0.620\n",
      "| steps 0001800 | loss: 0.220\n",
      "| steps 0001800 | Validation Accuracy: 0.650\n",
      "| steps 0001900 | loss: 0.184\n",
      "| steps 0001900 | Validation Accuracy: 0.630\n",
      "| steps 0002000 | loss: 0.200\n",
      "| steps 0002000 | Validation Accuracy: 0.640\n",
      "| steps 0002100 | loss: 0.170\n",
      "| steps 0002100 | Validation Accuracy: 0.640\n",
      "| steps 0002200 | loss: 0.220\n",
      "| steps 0002200 | Validation Accuracy: 0.650\n",
      "| steps 0002300 | loss: 0.148\n",
      "| steps 0002300 | Validation Accuracy: 0.640\n",
      "| steps 0002400 | loss: 0.158\n",
      "| steps 0002400 | Validation Accuracy: 0.690\n",
      "| steps 0002500 | loss: 0.149\n",
      "| steps 0002500 | Validation Accuracy: 0.720\n",
      "| steps 0002600 | loss: 0.213\n",
      "| steps 0002600 | Validation Accuracy: 0.670\n",
      "| steps 0002700 | loss: 0.158\n",
      "| steps 0002700 | Validation Accuracy: 0.680\n",
      "| steps 0002800 | loss: 0.102\n",
      "| steps 0002800 | Validation Accuracy: 0.710\n",
      "| steps 0002900 | loss: 0.123\n",
      "| steps 0002900 | Validation Accuracy: 0.710\n",
      "| steps 0003000 | loss: 0.134\n",
      "| steps 0003000 | Validation Accuracy: 0.720\n",
      "| steps 0003100 | loss: 0.126\n",
      "| steps 0003100 | Validation Accuracy: 0.670\n",
      "| steps 0003200 | loss: 0.242\n",
      "| steps 0003200 | Validation Accuracy: 0.670\n",
      "| steps 0003300 | loss: 0.193\n",
      "| steps 0003300 | Validation Accuracy: 0.690\n",
      "| steps 0003400 | loss: 0.099\n",
      "| steps 0003400 | Validation Accuracy: 0.790\n",
      "| steps 0003500 | loss: 0.126\n",
      "| steps 0003500 | Validation Accuracy: 0.780\n",
      "| steps 0003600 | loss: 0.104\n",
      "| steps 0003600 | Validation Accuracy: 0.770\n",
      "| steps 0003700 | loss: 0.122\n",
      "| steps 0003700 | Validation Accuracy: 0.780\n",
      "| steps 0003800 | loss: 0.099\n",
      "| steps 0003800 | Validation Accuracy: 0.800\n",
      "| steps 0003900 | loss: 0.126\n",
      "| steps 0003900 | Validation Accuracy: 0.770\n",
      "| steps 0004000 | loss: 0.157\n",
      "| steps 0004000 | Validation Accuracy: 0.720\n",
      "| steps 0004100 | loss: 0.117\n",
      "| steps 0004100 | Validation Accuracy: 0.780\n",
      "| steps 0004200 | loss: 0.124\n",
      "| steps 0004200 | Validation Accuracy: 0.790\n",
      "| steps 0004300 | loss: 0.057\n",
      "| steps 0004300 | Validation Accuracy: 0.770\n",
      "| steps 0004400 | loss: 0.197\n",
      "| steps 0004400 | Validation Accuracy: 0.790\n",
      "| steps 0004500 | loss: 0.123\n",
      "| steps 0004500 | Validation Accuracy: 0.790\n",
      "| steps 0004600 | loss: 0.176\n",
      "| steps 0004600 | Validation Accuracy: 0.770\n",
      "| steps 0004700 | loss: 0.177\n",
      "| steps 0004700 | Validation Accuracy: 0.780\n",
      "| steps 0004800 | loss: 0.096\n",
      "| steps 0004800 | Validation Accuracy: 0.800\n",
      "| steps 0004900 | loss: 0.233\n",
      "| steps 0004900 | Validation Accuracy: 0.790\n",
      "| steps 0005000 | loss: 0.099\n",
      "| steps 0005000 | Validation Accuracy: 0.790\n",
      "| steps 0005100 | loss: 0.187\n",
      "| steps 0005100 | Validation Accuracy: 0.800\n",
      "| steps 0005200 | loss: 0.149\n",
      "| steps 0005200 | Validation Accuracy: 0.790\n",
      "| steps 0005300 | loss: 0.210\n",
      "| steps 0005300 | Validation Accuracy: 0.790\n",
      "| steps 0005400 | loss: 0.183\n",
      "| steps 0005400 | Validation Accuracy: 0.780\n",
      "| steps 0005500 | loss: 0.091\n",
      "| steps 0005500 | Validation Accuracy: 0.770\n",
      "| steps 0005600 | loss: 0.193\n",
      "| steps 0005600 | Validation Accuracy: 0.790\n",
      "| steps 0005700 | loss: 0.126\n",
      "| steps 0005700 | Validation Accuracy: 0.780\n",
      "| steps 0005800 | loss: 0.201\n",
      "| steps 0005800 | Validation Accuracy: 0.780\n",
      "| steps 0005900 | loss: 0.255\n",
      "| steps 0005900 | Validation Accuracy: 0.780\n",
      "| steps 0006000 | loss: 0.154\n",
      "| steps 0006000 | Validation Accuracy: 0.780\n",
      "| steps 0006100 | loss: 0.204\n",
      "| steps 0006100 | Validation Accuracy: 0.790\n",
      "| steps 0006200 | loss: 0.164\n",
      "| steps 0006200 | Validation Accuracy: 0.790\n",
      "| steps 0006300 | loss: 0.072\n",
      "| steps 0006300 | Validation Accuracy: 0.780\n",
      "| steps 0006400 | loss: 0.106\n",
      "| steps 0006400 | Validation Accuracy: 0.780\n",
      "| steps 0006500 | loss: 0.149\n",
      "| steps 0006500 | Validation Accuracy: 0.790\n",
      "| steps 0006600 | loss: 0.163\n",
      "| steps 0006600 | Validation Accuracy: 0.790\n",
      "| steps 0006700 | loss: 0.195\n",
      "| steps 0006700 | Validation Accuracy: 0.790\n",
      "| steps 0006800 | loss: 0.147\n",
      "| steps 0006800 | Validation Accuracy: 0.780\n",
      "| steps 0006900 | loss: 0.131\n",
      "| steps 0006900 | Validation Accuracy: 0.790\n",
      "| steps 0007000 | loss: 0.107\n",
      "| steps 0007000 | Validation Accuracy: 0.790\n",
      "| steps 0007100 | loss: 0.113\n",
      "| steps 0007100 | Validation Accuracy: 0.790\n",
      "| steps 0007200 | loss: 0.110\n",
      "| steps 0007200 | Validation Accuracy: 0.800\n",
      "| steps 0007300 | loss: 0.125\n",
      "| steps 0007300 | Validation Accuracy: 0.790\n",
      "| steps 0007400 | loss: 0.210\n",
      "| steps 0007400 | Validation Accuracy: 0.790\n",
      "| steps 0007500 | loss: 0.240\n",
      "| steps 0007500 | Validation Accuracy: 0.790\n",
      "| steps 0007600 | loss: 0.231\n",
      "| steps 0007600 | Validation Accuracy: 0.780\n",
      "| steps 0007700 | loss: 0.142\n",
      "| steps 0007700 | Validation Accuracy: 0.790\n",
      "| steps 0007800 | loss: 0.115\n",
      "| steps 0007800 | Validation Accuracy: 0.780\n",
      "| steps 0007900 | loss: 0.068\n",
      "| steps 0007900 | Validation Accuracy: 0.780\n",
      "| steps 0008000 | loss: 0.124\n",
      "| steps 0008000 | Validation Accuracy: 0.780\n",
      "| steps 0008100 | loss: 0.145\n",
      "| steps 0008100 | Validation Accuracy: 0.790\n",
      "| steps 0008200 | loss: 0.075\n",
      "| steps 0008200 | Validation Accuracy: 0.780\n",
      "| steps 0008300 | loss: 0.102\n",
      "| steps 0008300 | Validation Accuracy: 0.790\n",
      "| steps 0008400 | loss: 0.111\n",
      "| steps 0008400 | Validation Accuracy: 0.780\n",
      "| steps 0008500 | loss: 0.213\n",
      "| steps 0008500 | Validation Accuracy: 0.780\n",
      "| steps 0008600 | loss: 0.163\n",
      "| steps 0008600 | Validation Accuracy: 0.790\n",
      "| steps 0008700 | loss: 0.248\n",
      "| steps 0008700 | Validation Accuracy: 0.780\n",
      "| steps 0008800 | loss: 0.108\n",
      "| steps 0008800 | Validation Accuracy: 0.790\n",
      "| steps 0008900 | loss: 0.181\n",
      "| steps 0008900 | Validation Accuracy: 0.770\n",
      "| steps 0009000 | loss: 0.082\n",
      "| steps 0009000 | Validation Accuracy: 0.770\n",
      "| steps 0009100 | loss: 0.113\n",
      "| steps 0009100 | Validation Accuracy: 0.800\n",
      "| steps 0009200 | loss: 0.146\n",
      "| steps 0009200 | Validation Accuracy: 0.770\n",
      "| steps 0009300 | loss: 0.083\n",
      "| steps 0009300 | Validation Accuracy: 0.800\n",
      "| steps 0009400 | loss: 0.130\n",
      "| steps 0009400 | Validation Accuracy: 0.790\n",
      "| steps 0009500 | loss: 0.023\n",
      "| steps 0009500 | Validation Accuracy: 0.780\n",
      "| steps 0009600 | loss: 0.159\n",
      "| steps 0009600 | Validation Accuracy: 0.800\n",
      "| steps 0009700 | loss: 0.074\n",
      "| steps 0009700 | Validation Accuracy: 0.790\n",
      "| steps 0009800 | loss: 0.063\n",
      "| steps 0009800 | Validation Accuracy: 0.800\n",
      "| steps 0009900 | loss: 0.065\n",
      "| steps 0009900 | Validation Accuracy: 0.790\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Loading data reader at Practice(2)\n",
    "data_reader = reader()\n",
    "\n",
    "# Loading DNN graph at Practice(3)\n",
    "model = dnn()\n",
    "\n",
    "# Define Session for running graph\n",
    "# and initialize model's parameters\n",
    "sess = tf.Session()\n",
    "\n",
    "# Define op merging all summaries\n",
    "summary_register(model.scalar_summaries, model.histogram_summaries)\n",
    "merge_op = tf.summary.merge_all()\n",
    "\n",
    "# Define Summary Writer for both training and validation phase\n",
    "train_summary_writer= tf.summary.FileWriter(\"./logs/train\")\n",
    "val_summary_writer= tf.summary.FileWriter(\"./logs/val\")\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "batch_size = 16\n",
    "max_steps = 10000\n",
    "\n",
    "\n",
    "for i in range(max_steps):\n",
    "  # For each iteration, first we get batch x&y data\n",
    "  x_train, y_train, id_train = data_reader.next_batch(16, split=\"train\")\n",
    "\n",
    "  # Next, construct feed for model's placeholder\n",
    "  # feed is dictionary whose key is placeholder, and value is feeded value(numpy array)\n",
    "  feed = {model.x: x_train, model.y: y_train}\n",
    "\n",
    "  # Go training via running train_op with feeded data!\n",
    "  # We run simultaneously train_op(backprop) and loss value\n",
    "  _, loss = sess.run([model.train_op, model.loss], feed_dict=feed)\n",
    "\n",
    "  if i%10 == 0:\n",
    "    # Writing training phase summary\n",
    "    summary = sess.run(merge_op, feed_dict=feed)\n",
    "    train_summary_writer.add_summary(summary, i)\n",
    "\n",
    "\n",
    "  # print loss stat every 100 iterations\n",
    "  if i%100 == 0:\n",
    "    print \"| steps %07d | loss: %.3lf\" % (i, loss)\n",
    "\n",
    "\n",
    "#   running validation process every 100 iterations\n",
    "  if i%100 == 0:\n",
    "    x_val, y_val, id_val = data_reader.next_batch(100, split=\"val\")\n",
    "    feed_val = {model.x: x_val, model.y: y_val}\n",
    "\n",
    "    validation_acc, summary = sess.run([model.acc, merge_op], feed_dict=feed_val)\n",
    "\n",
    "    # Writing validation phase summary\n",
    "    val_summary_writer.add_summary(summary, i)\n",
    "    print \"| steps %07d | Validation Accuracy: %.3lf\" % (i, validation_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
