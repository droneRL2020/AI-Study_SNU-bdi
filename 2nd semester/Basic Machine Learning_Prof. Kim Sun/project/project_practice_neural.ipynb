{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "class reader(object):\n",
    "  def __init__(self, data_file = \"./data/wdbc.data\"):\n",
    "    self.value = []\n",
    "    with open(data_file, \"rb\") as f:\n",
    "      csv_reader = csv.reader(f, delimiter=\",\")\n",
    "      for i, row in enumerate(csv_reader):\n",
    "        self.value.append(row)\n",
    "    # print self.value[0]\n",
    "    # print self.value[0][0]\n",
    "    # print self.value[0][1]\n",
    "\n",
    "    self.raw_to_vector(self.value)\n",
    "    self.split(num_validation_examples=114)\n",
    "    \n",
    "#     print self.id.shape\n",
    "#     print len(self.x)\n",
    "#     print self.y.shape\n",
    "    \n",
    "    self.num_examples = len(self.x_train)\n",
    "    self.start_index = 0\n",
    "    self.shuffle_indices = range(self.num_examples)\n",
    "\n",
    "    self.num_examples_val = len(self.x_val) # = 114 (validation_data)\n",
    "    self.start_index_val = 0\n",
    "    self.shuffle_indices_val = range(self.num_examples_val)\n",
    "    print self.num_examples_val\n",
    "\n",
    "  def raw_to_vector(self, value):\n",
    "    self.id = []\n",
    "    self.x = []\n",
    "    self.y = []\n",
    "\n",
    "    for row in self.value:\n",
    "        x = np.zeros(30)\n",
    "        for i in range(30):\n",
    "            x[i] = float(row[i+2])\n",
    "        if row[1] == \"B\":\n",
    "            y = 0\n",
    "        else:\n",
    "            y = 1\n",
    "        self.x.append(x)\n",
    "        self.y.append(y)\n",
    "        id = int(row[0])\n",
    "        self.id.append(id)\n",
    "        \n",
    "    self.x, self.y, self.id = np.array(self.x), np.array(self.y), np.array(self.id)\n",
    "\n",
    "  def split(self, num_validation_examples):\n",
    "    self.x_train = self.x[ num_validation_examples: ]\n",
    "    self.x_val = self.x[ : num_validation_examples ]\n",
    "\n",
    "    self.y_train = self.y[ num_validation_examples: ]\n",
    "    self.y_val = self.y[ : num_validation_examples ]\n",
    "\n",
    "    self.id_train = self.id[ num_validation_examples: ]\n",
    "    self.id_val = self.id[ :num_validation_examples ]\n",
    "\n",
    "  def next_batch(self, batch_size, split=\"train\"):\n",
    "\n",
    "    if split == \"train\":\n",
    "      if self.start_index == 0:\n",
    "        np.random.shuffle(self.shuffle_indices) # shuffle indices\n",
    "\n",
    "      end_index = min([self.num_examples, self.start_index + batch_size])\n",
    "      batch_indices = [ self.shuffle_indices[idx] for idx in range(self.start_index, end_index) ]\n",
    "\n",
    "      batch_x = self.x_train[ batch_indices ]\n",
    "      batch_y = self.y_train[ batch_indices ]\n",
    "      batch_id = self.id_train[ batch_indices ] \n",
    "\n",
    "      if end_index == self.num_examples:\n",
    "        self.start_index = 0\n",
    "      else: self.start_index = end_index\n",
    "\n",
    "      return batch_x, batch_y, batch_id \n",
    "\n",
    "    elif split == \"val\":\n",
    "      if self.start_index_val == 0:\n",
    "        np.random.shuffle(self.shuffle_indices_val) # shuffle indices\n",
    "\n",
    "      end_index = min([self.num_examples_val, self.start_index_val + batch_size])\n",
    "      batch_indices = [ self.shuffle_indices_val[idx] for idx in range(self.start_index_val, end_index) ]\n",
    "\n",
    "      batch_x = self.x_val[ batch_indices ]\n",
    "      batch_y = self.y_val[ batch_indices ]\n",
    "      batch_id = self.id_val[ batch_indices ] \n",
    "\n",
    "      if end_index == self.num_examples_val:\n",
    "        self.start_index_val = 0\n",
    "      else: self.start_index_val = end_index\n",
    "\n",
    "      return batch_x, batch_y, batch_id  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class dnn(object):\n",
    "  def __init__(self):\n",
    "    # Define model's input as tf.placeholder\n",
    "    self.x = tf.placeholder(dtype=tf.float32, shape=[None, 30])\n",
    "    self.y = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "\n",
    "    \n",
    "    # Define parameters for 3-layers dnn model\n",
    "    self.w_1, self.b_1 = tf.get_variable(name=\"w_1\", shape=[30, 50]), tf.get_variable(name=\"b_1\", shape=[50])\n",
    "    self.w_2, self.b_2 = tf.get_variable(name=\"w_2\", shape=[50, 50]), tf.get_variable(name=\"b_2\", shape=[50])\n",
    "    self.w_3, self.b_3 = tf.get_variable(name=\"w_3\", shape=[50, 2]), tf.get_variable(name=\"b_3\", shape=[2])\n",
    "    self.build_graph()\n",
    "\n",
    "  def build_graph(self):\n",
    "\n",
    "    # h1.shape = (batch, 50)\n",
    "    h1 = tf.matmul(self.x, self.w_1) + self.b_1 \n",
    "    h1 = tf.nn.relu(h1)\n",
    "\n",
    "    # h2.shape = (batch, 50)\n",
    "    h2 = tf.matmul(h1, self.w_2) + self.b_2\n",
    "    h2 = tf.nn.relu(h2)\n",
    "\n",
    "    # h3.shape = (batch, 2)\n",
    "    h3 = tf.matmul(h2, self.w_3) + self.b_3\n",
    "    h3 = tf.nn.softmax(h3, dim=-1)\n",
    "\n",
    "    prediction = h3\n",
    "\n",
    "    y_onehot = tf.one_hot(\n",
    "      indices=self.y,\n",
    "      depth=2,\n",
    "      on_value=1.0,\n",
    "      off_value=0.0,\n",
    "      )\n",
    "\n",
    "    # We use L2 loss as cost function and average the batch's loss\n",
    "    self.loss = tf.reduce_mean((y_onehot - prediction) ** 2)\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    \n",
    "    self.train_op = optimizer.minimize(self.loss)\n",
    "\n",
    "    pred_index = tf.cast(tf.argmax(prediction, dimension=1), tf.int32)\n",
    "    correct_prediction = tf.equal(pred_index, self.y)\n",
    "    self.acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print self.acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114\n",
      "Tensor(\"Mean_1:0\", shape=(), dtype=float32)\n",
      "| steps 0000000 | loss: 0.250\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "float argument required, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-dc2aa019843b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# Writing validation phase summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"| steps %07d | Validation Accuracy: %.3lf\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: float argument required, not list"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Loading data reader at Practice(2)\n",
    "data_reader = reader()\n",
    "\n",
    "# Loading DNN graph at Practice(3)\n",
    "model = dnn()\n",
    "\n",
    "# Define Session for running graph\n",
    "# and initialize model's parameters\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "batch_size = 16\n",
    "max_steps = 10000\n",
    "\n",
    "\n",
    "for i in range(max_steps):\n",
    "  # For each iteration, first we get batch x&y data\n",
    "  x_train, y_train, id_train = data_reader.next_batch(16)\n",
    "\n",
    "  # Next, construct feed for model's placeholder\n",
    "  # feed is dictionary whose key is placeholder, and value is feeded value(numpy array)\n",
    "  feed = {model.x: x_train, model.y: y_train}\n",
    "\n",
    "  # Go training via running train_op with feeded data!\n",
    "  # We run simultaneously train_op(backprop) and loss value\n",
    "  _, loss = sess.run([model.train_op, model.loss], feed_dict=feed)\n",
    "\n",
    "\n",
    "  # print loss stat every 100 iterations\n",
    "  if i%100 == 0:\n",
    "    print \"| steps %07d | loss: %.3lf\" % (i, loss)\n",
    "\n",
    "#   running validation process every 100 iterations\n",
    "  if i%100 == 0:\n",
    "    x_val, y_val, id_val = data_reader.next_batch(50, split=\"val\")\n",
    "    feed_val = {model.x: x_val, model.y: y_val}\n",
    "\n",
    "    validation_acc= sess.run([model.acc], feed_dict=feed_val)\n",
    "\n",
    "    # Writing validation phase summary\n",
    "    print \"| steps %07d | Validation Accuracy: %.3lf\" % (i, validation_acc)\n",
    "    print model.acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
